from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from datetime import datetime
import uuid
import logging
import random

class TwitterScraper:
    def __init__(self, username, password):
        self.username = username
        self.password = password
        self.setup_logging()
        
    def setup_logging(self):
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        
    def get_proxy(self):
        """Get a random ProxyMesh proxy."""
        # Replace with your ProxyMesh credentials and proxies
        proxies = [
            "us-wa.proxymesh.com:31280",
            "us-ny.proxymesh.com:31280",
            "us-fl.proxymesh.com:31280"
        ]
        return random.choice(proxies)
        
    def setup_driver(self):
        proxy = self.get_proxy()
        options = webdriver.ChromeOptions()
        options.add_argument(f'--proxy-server={proxy}')
        self.driver = webdriver.Chrome(options=options)
        return proxy
        
    def login_twitter(self):
        """Login to Twitter account."""
        self.driver.get("https://twitter.com/login")
        wait = WebDriverWait(self.driver, 10)
        
        # Input username
        username_field = wait.until(
            EC.presence_of_element_located((By.xpath, "//input[@autocomplete='username']"))
        )
        username_field.send_keys(self.username)
        self.driver.find_element(By.xpath, "//span[text()='Next']").click()
        
        # Input password
        password_field = wait.until(
            EC.presence_of_element_located((By.xpath, "//input[@name='password']"))
        )
        password_field.send_keys(self.password)
        self.driver.find_element(By.xpath, "//span[text()='Log in']").click()
        
    def get_trending_topics(self):
        """Scrape trending topics and return results with metadata."""
        unique_id = str(uuid.uuid4())
        proxy_ip = self.get_proxy()
        trends = []
        
        try:
            proxy_ip = self.setup_driver()
            self.login_twitter()
            
            # Wait for trends to load
            wait = WebDriverWait(self.driver, 10)
            trend_elements = wait.until(
                EC.presence_of_all_elements_located((By.css_selector, '[data-testid="trend"]'))
            )
            
            # Get top 5 trends
            for element in trend_elements[:5]:
                trend_text = element.text.split('\n')[0]
                trends.append(trend_text)
                
        except Exception as e:
            self.logger.error(f"Error scraping trends: {str(e)}")
            raise
        finally:
            end_time = datetime.now()
            self.driver.quit()
            
        return {
            "_id": unique_id,
            "nameoftrend1": trends[0] if len(trends) > 0 else "",
            "nameoftrend2": trends[1] if len(trends) > 1 else "",
            "nameoftrend3": trends[2] if len(trends) > 2 else "",
            "nameoftrend4": trends[3] if len(trends) > 3 else "",
            "nameoftrend5": trends[4] if len(trends) > 4 else "",
            "timestamp": end_time,
            "ip_address": proxy_ip
        }
